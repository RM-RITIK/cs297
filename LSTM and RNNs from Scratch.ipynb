{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba18e621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A single sample from the generated dataset:\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "def generate_dataset(num_sequences=2**8):\n",
    "    \"\"\"\n",
    "    Generates a number of sequences as our dataset.\n",
    "    \n",
    "    Args:\n",
    "     `num_sequences`: the number of sequences to be generated.\n",
    "     \n",
    "    Returns a list of sequences.\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "    \n",
    "    for _ in range(num_sequences): \n",
    "        num_tokens = np.random.randint(1, 12)\n",
    "        sample = ['a'] * num_tokens + ['b'] * num_tokens + ['EOS']\n",
    "        samples.append(sample)\n",
    "        \n",
    "    return samples\n",
    "\n",
    "\n",
    "sequences = generate_dataset()\n",
    "\n",
    "print('A single sample from the generated dataset:')\n",
    "print(sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44dfd4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 256 sentences and 4 unique tokens in our dataset (including UNK).\n",
      "\n",
      "The index of 'b' is 1\n",
      "The word corresponding to index 1 is 'b'\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def sequences_to_dicts(sequences):\n",
    "    flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "    \n",
    "    all_words = flatten(sequences)\n",
    "    \n",
    "    # Count number of word occurences\n",
    "    word_count = defaultdict(int)\n",
    "    for word in flatten(sequences):\n",
    "        word_count[word] += 1\n",
    "\n",
    "    # Sort by frequency\n",
    "    word_count = sorted(list(word_count.items()), key=lambda l: -l[1])\n",
    "\n",
    "    # Create a list of all unique words\n",
    "    unique_words = [item[0] for item in word_count]\n",
    "    \n",
    "    # Add UNK token to list of words\n",
    "    unique_words.append('UNK')\n",
    "\n",
    "    # Count number of sequences and number of unique words\n",
    "    num_sentences, vocab_size = len(sequences), len(unique_words)\n",
    "\n",
    "    # Create dictionaries so that we can go from word to index and back\n",
    "    # If a word is not in our vocabulary, we assign it to token 'UNK'\n",
    "    word_to_idx = defaultdict(lambda: vocab_size-1)\n",
    "    idx_to_word = defaultdict(lambda: 'UNK')\n",
    "\n",
    "    # Fill dictionaries\n",
    "    for idx, word in enumerate(unique_words):\n",
    "        word_to_idx[word] = idx\n",
    "        idx_to_word[idx] = word\n",
    "\n",
    "    return word_to_idx, idx_to_word, num_sentences, vocab_size\n",
    "\n",
    "\n",
    "word_to_idx, idx_to_word, num_sequences, vocab_size = sequences_to_dicts(sequences)\n",
    "\n",
    "print(f'We have {num_sequences} sentences and {len(word_to_idx)} unique tokens in our dataset (including UNK).\\n')\n",
    "print('The index of \\'b\\' is', word_to_idx['b'])\n",
    "print(f'The word corresponding to index 1 is \\'{idx_to_word[1]}\\'')\n",
    "\n",
    "assert idx_to_word[word_to_idx['b']] == 'b', \\\n",
    "    'Consistency error: something went wrong in the conversion.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61fe29fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 204 samples in the training set.\n",
      "We have 25 samples in the validation set.\n",
      "We have 25 samples in the test set.\n"
     ]
    }
   ],
   "source": [
    "from torch.utils import data\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, inputs, targets):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the size of the dataset\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Retrieve inputs and targets at the given index\n",
    "        X = self.inputs[index]\n",
    "        y = self.targets[index]\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    \n",
    "def create_datasets(sequences, dataset_class, p_train=0.8, p_val=0.1, p_test=0.1):\n",
    "    # Define partition sizes\n",
    "    num_train = int(len(sequences)*p_train)\n",
    "    num_val = int(len(sequences)*p_val)\n",
    "    num_test = int(len(sequences)*p_test)\n",
    "\n",
    "    # Split sequences into partitions\n",
    "    sequences_train = sequences[:num_train]\n",
    "    sequences_val = sequences[num_train:num_train+num_val]\n",
    "    sequences_test = sequences[-num_test:]\n",
    "\n",
    "    def get_inputs_targets_from_sequences(sequences):\n",
    "        # Define empty lists\n",
    "        inputs, targets = [], []\n",
    "        \n",
    "        # Append inputs and targets s.t. both lists contain L-1 words of a sentence of length L\n",
    "        # but targets are shifted right by one so that we can predict the next word\n",
    "        for sequence in sequences:\n",
    "            inputs.append(sequence[:-1])\n",
    "            targets.append(sequence[1:])\n",
    "            \n",
    "        return inputs, targets\n",
    "\n",
    "    # Get inputs and targets for each partition\n",
    "    inputs_train, targets_train = get_inputs_targets_from_sequences(sequences_train)\n",
    "    inputs_val, targets_val = get_inputs_targets_from_sequences(sequences_val)\n",
    "    inputs_test, targets_test = get_inputs_targets_from_sequences(sequences_test)\n",
    "\n",
    "    # Create datasets\n",
    "    training_set = dataset_class(inputs_train, targets_train)\n",
    "    validation_set = dataset_class(inputs_val, targets_val)\n",
    "    test_set = dataset_class(inputs_test, targets_test)\n",
    "\n",
    "    return training_set, validation_set, test_set\n",
    "    \n",
    "\n",
    "training_set, validation_set, test_set = create_datasets(sequences, Dataset)\n",
    "\n",
    "print(f'We have {len(training_set)} samples in the training set.')\n",
    "print(f'We have {len(validation_set)} samples in the validation set.')\n",
    "print(f'We have {len(test_set)} samples in the test set.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a70cc9a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our one-hot encoding of 'a' has shape (4,).\n",
      "Our one-hot encoding of 'a b' has shape (2, 4, 1).\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(idx, vocab_size):\n",
    "    \"\"\"\n",
    "    One-hot encodes a single word given its index and the size of the vocabulary.\n",
    "    \n",
    "    Args:\n",
    "     `idx`: the index of the given word\n",
    "     `vocab_size`: the size of the vocabulary\n",
    "    \n",
    "    Returns a 1-D numpy array of length `vocab_size`.\n",
    "    \"\"\"\n",
    "    # Initialize the encoded array\n",
    "    one_hot = np.zeros(vocab_size)\n",
    "    \n",
    "    # Set the appropriate element to one\n",
    "    one_hot[idx] = 1.0\n",
    "\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "def one_hot_encode_sequence(sequence, vocab_size):\n",
    "    \"\"\"\n",
    "    One-hot encodes a sequence of words given a fixed vocabulary size.\n",
    "    \n",
    "    Args:\n",
    "     `sentence`: a list of words to encode\n",
    "     `vocab_size`: the size of the vocabulary\n",
    "     \n",
    "    Returns a 3-D numpy array of shape (num words, vocab size, 1).\n",
    "    \"\"\"\n",
    "    # Encode each word in the sentence\n",
    "    encoding = np.array([one_hot_encode(word_to_idx[word], vocab_size) for word in sequence])\n",
    "\n",
    "    # Reshape encoding s.t. it has shape (num words, vocab size, 1)\n",
    "    encoding = encoding.reshape(encoding.shape[0], encoding.shape[1], 1)\n",
    "    \n",
    "    return encoding\n",
    "\n",
    "\n",
    "test_word = one_hot_encode(word_to_idx['a'], vocab_size)\n",
    "print(f'Our one-hot encoding of \\'a\\' has shape {test_word.shape}.')\n",
    "\n",
    "test_sentence = one_hot_encode_sequence(['a', 'b'], vocab_size)\n",
    "print(f'Our one-hot encoding of \\'a b\\' has shape {test_sentence.shape}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd5a60f",
   "metadata": {},
   "source": [
    "## RNN Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "814f8379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U: (50, 4)\n",
      "V: (50, 50)\n",
      "W: (4, 50)\n",
      "b_hidden: (50, 1)\n",
      "b_out: (4, 1)\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 50 # Number of dimensions in the hidden state\n",
    "vocab_size  = len(word_to_idx) # Size of the vocabulary used\n",
    "\n",
    "def init_orthogonal(param):\n",
    "    \"\"\"\n",
    "    Initializes weight parameters orthogonally.\n",
    "    This is a common initiailization for recurrent neural networks.\n",
    "    \n",
    "    Refer to this paper for an explanation of this initialization:\n",
    "    https://arxiv.org/abs/1312.6120\n",
    "    \"\"\"\n",
    "    if param.ndim < 2:\n",
    "        raise ValueError(\"Only parameters with 2 or more dimensions are supported.\")\n",
    "\n",
    "    rows, cols = param.shape\n",
    "    \n",
    "    new_param = np.random.randn(rows, cols)\n",
    "    \n",
    "    if rows < cols:\n",
    "        new_param = new_param.T\n",
    "    \n",
    "    # Compute QR factorization\n",
    "    q, r = np.linalg.qr(new_param)\n",
    "    \n",
    "    # Make Q uniform according to https://arxiv.org/pdf/math-ph/0609050.pdf\n",
    "    d = np.diag(r, 0)\n",
    "    ph = np.sign(d)\n",
    "    q *= ph\n",
    "\n",
    "    if rows < cols:\n",
    "        q = q.T\n",
    "    \n",
    "    new_param = q\n",
    "    \n",
    "    return new_param\n",
    "\n",
    "\n",
    "def init_rnn(hidden_size, vocab_size):\n",
    "    \"\"\"\n",
    "    Initializes our recurrent neural network.\n",
    "    \n",
    "    Args:\n",
    "     `hidden_size`: the dimensions of the hidden state\n",
    "     `vocab_size`: the dimensions of our vocabulary\n",
    "    \"\"\"\n",
    "    # Weight matrix (input to hidden state)\n",
    "    U = np.zeros((hidden_size, vocab_size))\n",
    "\n",
    "    # Weight matrix (recurrent computation)\n",
    "    V = np.zeros((hidden_size, hidden_size))\n",
    "\n",
    "    # Weight matrix (hidden state to output)\n",
    "    W = np.zeros((vocab_size, hidden_size))\n",
    "\n",
    "    # Bias (hidden state)\n",
    "    b_hidden = np.zeros((hidden_size, 1))\n",
    "\n",
    "    # Bias (output)\n",
    "    b_out = np.zeros((vocab_size, 1))\n",
    "    \n",
    "    # Initialize weights\n",
    "    U = init_orthogonal(U)\n",
    "    V = init_orthogonal(V)\n",
    "    W = init_orthogonal(W)\n",
    "    \n",
    "    # Return parameters as a tuple\n",
    "    return U, V, W, b_hidden, b_out\n",
    "\n",
    "\n",
    "params = init_rnn(hidden_size=hidden_size, vocab_size=vocab_size)\n",
    "print('U:', params[0].shape)\n",
    "print('V:', params[1].shape)\n",
    "print('W:', params[2].shape)\n",
    "print('b_hidden:', params[3].shape)\n",
    "print('b_out:', params[4].shape)\n",
    "\n",
    "for param in params:\n",
    "    assert param.ndim == 2, \\\n",
    "        'all parameters should be 2-dimensional '\\\n",
    "        '(hint: a dimension can simply have size 1)'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7512e6ff",
   "metadata": {},
   "source": [
    "#### Implementing Basic Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48434657",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x, derivative=False):\n",
    "    \"\"\"\n",
    "    Computes the element-wise sigmoid activation function for an array x.\n",
    "\n",
    "    Args:\n",
    "     `x`: the array where the function is applied\n",
    "     `derivative`: if set to True will return the derivative instead of the forward pass\n",
    "    \"\"\"\n",
    "    x_safe = x + 1e-12\n",
    "    f = 1 / (1 + np.exp(-x_safe))\n",
    "    \n",
    "    if derivative: # Return the derivative of the function evaluated at x\n",
    "        return f * (1 - f)\n",
    "    else: # Return the forward pass of the function at x\n",
    "        return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7bab0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x, derivative=False):\n",
    "    \"\"\"\n",
    "    Computes the element-wise tanh activation function for an array x.\n",
    "\n",
    "    Args:\n",
    "     `x`: the array where the function is applied\n",
    "     `derivative`: if set to True will return the derivative instead of the forward pass\n",
    "    \"\"\"\n",
    "    x_safe = x + 1e-12\n",
    "    f = (np.exp(x_safe)-np.exp(-x_safe))/(np.exp(x_safe)+np.exp(-x_safe))\n",
    "    \n",
    "    if derivative: # Return the derivative of the function evaluated at x\n",
    "        return 1-f**2\n",
    "    else: # Return the forward pass of the function at x\n",
    "        return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a11691dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, derivative=False):\n",
    "    \"\"\"\n",
    "    Computes the softmax for an array x.\n",
    "    \n",
    "    Args:\n",
    "     `x`: the array where the function is applied\n",
    "     `derivative`: if set to True will return the derivative instead of the forward pass\n",
    "    \"\"\"\n",
    "    x_safe = x + 1e-12\n",
    "    f = np.exp(x_safe) / np.sum(np.exp(x_safe))\n",
    "    \n",
    "    if derivative: # Return the derivative of the function evaluated at x\n",
    "        pass # We will not need this one\n",
    "    else: # Return the forward pass of the function at x\n",
    "        return f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693ab150",
   "metadata": {},
   "source": [
    "#### Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08ffcfff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequence:\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b']\n",
      "\n",
      "Target sequence:\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS']\n",
      "\n",
      "Predicted sequence:\n",
      "['UNK', 'b', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'a', 'a', 'a', 'b', 'b', 'a', 'b']\n"
     ]
    }
   ],
   "source": [
    "def forward_pass(inputs, hidden_state, params):\n",
    "    \"\"\"\n",
    "    Computes the forward pass of a vanilla RNN.\n",
    "    \n",
    "    Args:\n",
    "     `inputs`: sequence of inputs to be processed\n",
    "     `hidden_state`: an already initialized hidden state\n",
    "     `params`: the parameters of the RNN\n",
    "    \"\"\"\n",
    "    # First we unpack our parameters\n",
    "    U, V, W, b_hidden, b_out = params\n",
    "    \n",
    "    # Create a list to store outputs and hidden states\n",
    "    outputs, hidden_states = [], []\n",
    "    \n",
    "    # For each element in input sequence\n",
    "    for t in range(len(inputs)):\n",
    "\n",
    "        # Compute new hidden state\n",
    "        hidden_state_prev = hidden_states[t-1] if t > 0 else hidden_state\n",
    "        hidden_state = tanh(np.dot(U, inputs[t]) + np.dot(V, hidden_state_prev) + b_hidden)\n",
    "\n",
    "        # Compute output\n",
    "        out = softmax(np.dot(W, hidden_state) + b_out)\n",
    "        \n",
    "        # Save results and continue\n",
    "        outputs.append(out)\n",
    "        hidden_states.append(hidden_state.copy())\n",
    "    \n",
    "    return outputs, hidden_states\n",
    "\n",
    "\n",
    "# Get first sequence in training set\n",
    "test_input_sequence, test_target_sequence = training_set[0]\n",
    "\n",
    "# One-hot encode input and target sequence\n",
    "test_input = one_hot_encode_sequence(test_input_sequence, vocab_size)\n",
    "test_target = one_hot_encode_sequence(test_target_sequence, vocab_size)\n",
    "\n",
    "# Initialize hidden state as zeros\n",
    "hidden_state = np.zeros((hidden_size, 1))\n",
    "\n",
    "# Now let's try out our new function\n",
    "outputs, hidden_states = forward_pass(test_input, hidden_state, params)\n",
    "\n",
    "print('Input sequence:')\n",
    "print(test_input_sequence)\n",
    "\n",
    "print('\\nTarget sequence:')\n",
    "print(test_target_sequence)\n",
    "\n",
    "print('\\nPredicted sequence:')\n",
    "print([idx_to_word[np.argmax(output)] for output in outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d6b08f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_gradient_norm(grads, max_norm=0.25):\n",
    "    \"\"\"\n",
    "    Clips gradients to have a maximum norm of `max_norm`.\n",
    "    This is to prevent the exploding gradients problem.\n",
    "    \"\"\" \n",
    "    # Set the maximum of the norm to be of type float\n",
    "    max_norm = float(max_norm)\n",
    "    total_norm = 0\n",
    "    \n",
    "    # Calculate the L2 norm squared for each gradient and add them to the total norm\n",
    "    for grad in grads:\n",
    "        grad_norm = np.sum(np.power(grad, 2))\n",
    "        total_norm += grad_norm\n",
    "    \n",
    "    total_norm = np.sqrt(total_norm)\n",
    "    \n",
    "    # Calculate clipping coeficient\n",
    "    clip_coef = max_norm / (total_norm + 1e-6)\n",
    "    \n",
    "    # If the total norm is larger than the maximum allowable norm, then clip the gradient\n",
    "    if clip_coef < 1:\n",
    "        for grad in grads:\n",
    "            grad *= clip_coef\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4d984d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We get a loss of:\n",
      "19.442576294993177\n"
     ]
    }
   ],
   "source": [
    "def backward_pass(inputs, outputs, hidden_states, targets, params):\n",
    "    \"\"\"\n",
    "    Computes the backward pass of a vanilla RNN.\n",
    "    \n",
    "    Args:\n",
    "     `inputs`: sequence of inputs to be processed\n",
    "     `outputs`: sequence of outputs from the forward pass\n",
    "     `hidden_states`: sequence of hidden_states from the forward pass\n",
    "     `targets`: sequence of targets\n",
    "     `params`: the parameters of the RNN\n",
    "    \"\"\"\n",
    "    # First we unpack our parameters\n",
    "    U, V, W, b_hidden, b_out = params\n",
    "    \n",
    "    # Initialize gradients as zero\n",
    "    d_U, d_V, d_W = np.zeros_like(U), np.zeros_like(V), np.zeros_like(W)\n",
    "    d_b_hidden, d_b_out = np.zeros_like(b_hidden), np.zeros_like(b_out)\n",
    "    \n",
    "    # Keep track of hidden state derivative and loss\n",
    "    d_h_next = np.zeros_like(hidden_states[0])\n",
    "    loss = 0\n",
    "    \n",
    "    # For each element in output sequence\n",
    "    # NB: We iterate backwards s.t. t = N, N-1, ... 1, 0\n",
    "    for t in reversed(range(len(outputs))):\n",
    "\n",
    "        # Compute cross-entropy loss (as a scalar)\n",
    "        loss += -1*np.log(outputs[t][np.argmax(targets[t])][0])\n",
    "        \n",
    "        d_o = outputs[t].copy()\n",
    "        d_o[np.argmax(targets[t])] -= 1\n",
    "        \n",
    "        d_W += np.dot(d_o, hidden_states[t].T)\n",
    "        d_b_out += d_o\n",
    "        \n",
    "        d_h = np.dot(W.T, d_o) + d_h_next\n",
    "        \n",
    "        d_f = (1 - hidden_states[t]**2) * d_h\n",
    "        d_b_hidden += d_f\n",
    "        \n",
    "        d_U += np.dot(d_f, inputs[t].T)\n",
    "        \n",
    "        d_V += np.dot(d_f, hidden_states[t-1].T)\n",
    "        d_h_next = np.dot(V.T, d_f)\n",
    "    \n",
    "    # Pack gradients\n",
    "    grads = d_U, d_V, d_W, d_b_hidden, d_b_out    \n",
    "    \n",
    "    # Clip gradients\n",
    "    grads = clip_gradient_norm(grads)\n",
    "    \n",
    "    return loss, grads\n",
    "        \n",
    "\n",
    "\n",
    "loss, grads = backward_pass(test_input, outputs, hidden_states, test_target, params)\n",
    "\n",
    "print('We get a loss of:')\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84562891",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(params, grads, lr=1e-3):\n",
    "    # Take a step\n",
    "    for param, grad in zip(params, grads):\n",
    "        param -= lr * grad\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726c3b29",
   "metadata": {},
   "source": [
    "#### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14581f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, training loss: 15.910508980828963, validation loss: 16.77082373423666\n",
      "Epoch 100, training loss: 5.842587609551899, validation loss: 5.961775262985713\n",
      "Epoch 200, training loss: 4.837018727735779, validation loss: 5.1858526349135605\n",
      "Epoch 300, training loss: 4.597945355563288, validation loss: 5.042666304439337\n",
      "Epoch 400, training loss: 4.614525975998632, validation loss: 5.358800478112993\n",
      "Epoch 500, training loss: 4.688253280743628, validation loss: 5.525259372086577\n",
      "Epoch 600, training loss: 4.623338573450351, validation loss: 5.280416544872708\n",
      "Epoch 700, training loss: 4.432645988520303, validation loss: 5.06703114140339\n",
      "Epoch 800, training loss: 4.316693612553475, validation loss: 4.9507236393319065\n",
      "Epoch 900, training loss: 4.216705015299984, validation loss: 4.816942413797928\n",
      "Input sentence:\n",
      "['a', 'a', 'a', 'a', 'b', 'b', 'b', 'b']\n",
      "\n",
      "Target sequence:\n",
      "['a', 'a', 'a', 'b', 'b', 'b', 'b', 'EOS']\n",
      "\n",
      "Predicted sequence:\n",
      "['b', 'a', 'a', 'a', 'b', 'b', 'b', 'EOS']\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Hyper-parameters\n",
    "num_epochs = 1000\n",
    "\n",
    "# Initialize a new network\n",
    "params = init_rnn(hidden_size=hidden_size, vocab_size=vocab_size)\n",
    "\n",
    "# Initialize hidden state as zeros\n",
    "hidden_state = np.zeros((hidden_size, 1))\n",
    "\n",
    "# Track loss\n",
    "training_loss, validation_loss = [], []\n",
    "\n",
    "# For each epoch\n",
    "for i in range(num_epochs):\n",
    "    \n",
    "    # Track loss\n",
    "    epoch_training_loss = 0\n",
    "    epoch_validation_loss = 0\n",
    "    \n",
    "     # For each sentence in validation set\n",
    "    for inputs, targets in validation_set:\n",
    "        \n",
    "        # One-hot encode input and target sequence\n",
    "        inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size)\n",
    "        targets_one_hot = one_hot_encode_sequence(targets, vocab_size)\n",
    "        \n",
    "        # Re-initialize hidden state\n",
    "        hidden_state = np.zeros_like(hidden_state)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs, hidden_states = forward_pass(inputs_one_hot, hidden_state, params)\n",
    "\n",
    "        # Backward pass\n",
    "        loss, _ = backward_pass(inputs_one_hot, outputs, hidden_states, targets_one_hot, params)\n",
    "        \n",
    "        # Update loss\n",
    "        epoch_validation_loss += loss\n",
    "    \n",
    "    # For each sentence in training set\n",
    "    for inputs, targets in training_set:\n",
    "        \n",
    "        # One-hot encode input and target sequence\n",
    "        inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size)\n",
    "        targets_one_hot = one_hot_encode_sequence(targets, vocab_size)\n",
    "        \n",
    "        # Re-initialize hidden state\n",
    "        hidden_state = np.zeros_like(hidden_state)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs, hidden_states = forward_pass(inputs_one_hot, hidden_state, params)\n",
    "\n",
    "        # Backward pass\n",
    "        loss, grads = backward_pass(inputs_one_hot, outputs, hidden_states, targets_one_hot, params)\n",
    "        \n",
    "        if np.isnan(loss):\n",
    "            raise ValueError('Gradients have vanished/exploded!')\n",
    "        \n",
    "        # Update parameters\n",
    "        params = update_parameters(params, grads)\n",
    "        \n",
    "        # Update loss\n",
    "        epoch_training_loss += loss\n",
    "        \n",
    "    # Save loss for plot\n",
    "    training_loss.append(epoch_training_loss/len(training_set))\n",
    "    validation_loss.append(epoch_validation_loss/len(validation_set))\n",
    "\n",
    "    # Print loss every 100 epochs\n",
    "    if i % 100 == 0:\n",
    "        print(f'Epoch {i}, training loss: {training_loss[-1]}, validation loss: {validation_loss[-1]}')\n",
    "\n",
    "\n",
    "# Get first sentence in test set\n",
    "inputs, targets = test_set[1]\n",
    "\n",
    "# One-hot encode input and target sequence\n",
    "inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size)\n",
    "targets_one_hot = one_hot_encode_sequence(targets, vocab_size)\n",
    "\n",
    "# Initialize hidden state as zeros\n",
    "hidden_state = np.zeros((hidden_size, 1))\n",
    "\n",
    "# Forward pass\n",
    "outputs, hidden_states = forward_pass(inputs_one_hot, hidden_state, params)\n",
    "output_sentence = [idx_to_word[np.argmax(output)] for output in outputs]\n",
    "print('Input sentence:')\n",
    "print(inputs)\n",
    "\n",
    "print('\\nTarget sequence:')\n",
    "print(targets)\n",
    "\n",
    "print('\\nPredicted sequence:')\n",
    "print([idx_to_word[np.argmax(output)] for output in outputs])\n",
    "\n",
    "# Plot training and validation loss\n",
    "epoch = np.arange(len(training_loss))\n",
    "plt.figure()\n",
    "plt.plot(epoch, training_loss, 'r', label='Training loss',)\n",
    "plt.plot(epoch, validation_loss, 'b', label='Validation loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch'), plt.ylabel('NLL')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed0c66c",
   "metadata": {},
   "source": [
    "#### Extrapolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80047059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 0: a a b\n",
      "Predicted sequence: ['a', 'a', 'b', 'b', 'EOS']\n",
      "\n",
      "Example 1: a a a a b\n",
      "Predicted sequence: ['a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'a', 'a', 'a', 'a', 'a', 'a']\n",
      "\n",
      "Example 2: a a a a a a b\n",
      "Predicted sequence: ['a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS']\n",
      "\n",
      "Example 3: a\n",
      "Predicted sequence: ['a', 'b', 'EOS']\n",
      "\n",
      "Example 4: r n n\n",
      "Predicted sequence: ['r', 'n', 'n', 'EOS', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def freestyle(params, sentence='', num_generate=10):\n",
    "    \"\"\"\n",
    "    Takes in a sentence as a string and outputs a sequence\n",
    "    based on the predictions of the RNN.\n",
    "    \n",
    "    Args:\n",
    "     `params`: the parameters of the network\n",
    "     `sentence`: string with whitespace-separated tokens\n",
    "     `num_generate`: the number of tokens to generate\n",
    "    \"\"\"\n",
    "    sentence = sentence.split(' ')\n",
    "    \n",
    "    sentence_one_hot = one_hot_encode_sequence(sentence, vocab_size)\n",
    "    \n",
    "    # Initialize hidden state as zeros\n",
    "    hidden_state = np.zeros((hidden_size, 1))\n",
    "\n",
    "    # Generate hidden state for sentence\n",
    "    outputs, hidden_states = forward_pass(sentence_one_hot, hidden_state, params)\n",
    "    \n",
    "    # Output sentence\n",
    "    output_sentence = sentence\n",
    "    \n",
    "    # Append first prediction\n",
    "    word = idx_to_word[np.argmax(outputs[-1])]    \n",
    "    output_sentence.append(word)\n",
    "    \n",
    "    # Forward pass\n",
    "    for i in range(num_generate):\n",
    "\n",
    "        # Get the latest prediction and latest hidden state\n",
    "        output = outputs[-1]\n",
    "        hidden_state = hidden_states[-1]\n",
    "    \n",
    "        # Reshape our output to match the input shape of our forward pass\n",
    "        output = output.reshape(1, output.shape[0], output.shape[1])\n",
    "    \n",
    "        # Forward pass\n",
    "        outputs, hidden_states = forward_pass(output, hidden_state, params)\n",
    "        \n",
    "        # Compute the index of the most likely word and look up the corresponding word\n",
    "        word = idx_to_word[np.argmax(outputs)]\n",
    "        \n",
    "        output_sentence.append(word)\n",
    "        \n",
    "        if word == 'EOS':\n",
    "            break\n",
    "        \n",
    "    return output_sentence\n",
    "\n",
    "\n",
    "# Perform freestyle (extrapolation)\n",
    "test_examples = ['a a b', 'a a a a b', 'a a a a a a b', 'a', 'r n n']\n",
    "for i, test_example in enumerate(test_examples):\n",
    "    print(f'Example {i}:', test_example)\n",
    "    print('Predicted sequence:', freestyle(params, sentence=test_example), end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849f8bd1",
   "metadata": {},
   "source": [
    "## LSTMs\n",
    "\n",
    "A vanilla RNN suffers from vanishing gradient problem. So, the researchers turned their attention to LSTMs. There are three types of gates in LSTMs:\n",
    "\n",
    "<ul>\n",
    "    <li> Forget Gate: This gate decides which information needs attention and which can be ignored. </li>\n",
    "    <li> Input Gate </li>\n",
    "    <li> Output Gate </li>\n",
    "</ul>\n",
    "\n",
    "<img src=\"LSTM.png\" />\n",
    "<img src=\"LSTM2.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2e22c8",
   "metadata": {},
   "source": [
    "#### Initialization of LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8acfb9e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_f: (50, 54)\n",
      "W_i: (50, 54)\n",
      "W_g: (50, 54)\n",
      "W_o: (50, 54)\n",
      "W_v: (4, 50)\n",
      "b_i: (50, 1)\n",
      "b_g: (50, 1)\n",
      "b_o: (50, 1)\n",
      "b_v: (50, 1)\n"
     ]
    }
   ],
   "source": [
    "# Size of concatenated hidden + input vector\n",
    "z_size = hidden_size + vocab_size \n",
    "\n",
    "def init_lstm(hidden_size, vocab_size, z_size):\n",
    "    \"\"\"\n",
    "    Initializes our LSTM network.\n",
    "    \n",
    "    Args:\n",
    "     `hidden_size`: the dimensions of the hidden state\n",
    "     `vocab_size`: the dimensions of our vocabulary\n",
    "     `z_size`: the dimensions of the concatenated input \n",
    "    \"\"\"\n",
    "    # Weight matrix (forget gate)\n",
    "    W_f = np.zeros((hidden_size, z_size))\n",
    "    \n",
    "    # Bias for forget gate\n",
    "    b_f = np.zeros((hidden_size, 1))\n",
    "\n",
    "    # Weight matrix (input gate)\n",
    "    W_i = np.zeros((hidden_size, z_size))\n",
    "    \n",
    "    # Bias for input gate\n",
    "    b_i = np.zeros((hidden_size, 1))\n",
    "\n",
    "    # Weight matrix (candidate)\n",
    "    W_g = np.zeros((hidden_size, z_size))\n",
    "    \n",
    "    # Bias for candidate\n",
    "    b_g = np.zeros((hidden_size, 1))\n",
    "\n",
    "    # Weight matrix of the output gate\n",
    "    W_o = np.zeros((hidden_size, z_size))\n",
    "    \n",
    "    # Bias for output gate\n",
    "    b_o = np.zeros((hidden_size, 1))\n",
    "\n",
    "    # Weight matrix relating the hidden-state to the output\n",
    "    W_v = np.zeros((vocab_size, hidden_size))\n",
    "    \n",
    "    # Bias for logits\n",
    "    b_v = np.zeros((vocab_size, 1))\n",
    "    \n",
    "    # Initialize weights according to https://arxiv.org/abs/1312.6120\n",
    "    W_f = init_orthogonal(W_f)\n",
    "    W_i = init_orthogonal(W_i)\n",
    "    W_g = init_orthogonal(W_g)\n",
    "    W_o = init_orthogonal(W_o)\n",
    "    W_v = init_orthogonal(W_v)\n",
    "\n",
    "    return W_f, W_i, W_g, W_o, W_v, b_f, b_i, b_g, b_o, b_v\n",
    "\n",
    "\n",
    "params = init_lstm(hidden_size=hidden_size, vocab_size=vocab_size, z_size=z_size)\n",
    "print('W_f:', params[0].shape)\n",
    "print('W_i:', params[1].shape)\n",
    "print('W_g:', params[2].shape)\n",
    "print('W_o:', params[3].shape)\n",
    "print('W_v:', params[4].shape)\n",
    "print('b_i:', params[5].shape)\n",
    "print('b_g:', params[6].shape)\n",
    "print('b_o:', params[7].shape)\n",
    "print('b_v:', params[8].shape)\n",
    "\n",
    "for param in params:\n",
    "    assert param.ndim == 2, \\\n",
    "        'all parameters should be 2-dimensional '\\\n",
    "        '(hint: a dimension can simply have size 1)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "90a0d831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence:\n",
      "['a', 'a', 'a', 'a', 'b', 'b', 'b', 'b']\n",
      "\n",
      "Target sequence:\n",
      "['a', 'a', 'a', 'b', 'b', 'b', 'b', 'EOS']\n",
      "\n",
      "Predicted sequence:\n",
      "['UNK', 'UNK', 'UNK', 'UNK', 'EOS', 'EOS', 'EOS', 'EOS']\n"
     ]
    }
   ],
   "source": [
    "def forward(inputs, h_prev, C_prev, p):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    x -- your input data at timestep \"t\", numpy array of shape (n_x, m).\n",
    "    h_prev -- Hidden state at timestep \"t-1\", numpy array of shape (n_a, m)\n",
    "    C_prev -- Memory state at timestep \"t-1\", numpy array of shape (n_a, m)\n",
    "    p -- python list containing:\n",
    "                        W_f -- Weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        b_f -- Bias of the forget gate, numpy array of shape (n_a, 1)\n",
    "                        W_i -- Weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        b_i -- Bias of the update gate, numpy array of shape (n_a, 1)\n",
    "                        W_g -- Weight matrix of the first \"tanh\", numpy array of shape (n_a, n_a + n_x)\n",
    "                        b_g --  Bias of the first \"tanh\", numpy array of shape (n_a, 1)\n",
    "                        W_o -- Weight matrix of the output gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        b_o --  Bias of the output gate, numpy array of shape (n_a, 1)\n",
    "                        W_v -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_v, n_a)\n",
    "                        b_v -- Bias relating the hidden-state to the output, numpy array of shape (n_v, 1)\n",
    "    Returns:\n",
    "    z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s -- lists of size m containing the computations in each forward pass\n",
    "    outputs -- prediction at timestep \"t\", numpy array of shape (n_v, m)\n",
    "    \"\"\"\n",
    "    assert h_prev.shape == (hidden_size, 1)\n",
    "    assert C_prev.shape == (hidden_size, 1)\n",
    "\n",
    "    # First we unpack our parameters\n",
    "    W_f, W_i, W_g, W_o, W_v, b_f, b_i, b_g, b_o, b_v = p\n",
    "    \n",
    "    # Save a list of computations for each of the components in the LSTM\n",
    "    x_s, z_s, f_s, i_s,  = [], [] ,[], []\n",
    "    g_s, C_s, o_s, h_s = [], [] ,[], []\n",
    "    v_s, output_s =  [], [] \n",
    "    \n",
    "    # Append the initial cell and hidden state to their respective lists\n",
    "    h_s.append(h_prev)\n",
    "    C_s.append(C_prev)\n",
    "    \n",
    "    for x in inputs:\n",
    "        \n",
    "        # Concatenate input and hidden state\n",
    "        z = np.row_stack((h_prev, x))\n",
    "        z_s.append(z)\n",
    "        \n",
    "        # Calculate forget gate\n",
    "        f = softmax(np.dot(W_f, z) + b_f)\n",
    "        f_s.append(f)\n",
    "        \n",
    "        # Calculate input gate\n",
    "        i = softmax(np.dot(W_i, z) + b_i)\n",
    "        i_s.append(i)\n",
    "        \n",
    "        # Calculate candidate\n",
    "        g = tanh(np.dot(W_g, z) + b_g)\n",
    "        g_s.append(g)\n",
    "        \n",
    "        # Calculate memory state\n",
    "        C_prev = np.multiply(C_prev, f) + np.multiply(g, i)\n",
    "        C_s.append(C_prev)\n",
    "        \n",
    "        # Calculate output gate\n",
    "        o = softmax(np.dot(W_o, z) + b_o)\n",
    "        o_s.append(o)\n",
    "        \n",
    "        # Calculate hidden state\n",
    "        h_prev = o * tanh(C_prev)\n",
    "        h_s.append(h_prev)\n",
    "\n",
    "        # Calculate logits\n",
    "        v = np.dot(W_v, h_prev) + b_v\n",
    "        v_s.append(v)\n",
    "        \n",
    "        # Calculate softmax\n",
    "        output = softmax(v)\n",
    "        output_s.append(output)\n",
    "\n",
    "    return z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s, output_s\n",
    "\n",
    "\n",
    "# Get first sentence in test set\n",
    "inputs, targets = test_set[1]\n",
    "\n",
    "# One-hot encode input and target sequence\n",
    "inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size)\n",
    "targets_one_hot = one_hot_encode_sequence(targets, vocab_size)\n",
    "\n",
    "# Initialize hidden state as zeros\n",
    "h = np.zeros((hidden_size, 1))\n",
    "c = np.zeros((hidden_size, 1))\n",
    "\n",
    "# Forward pass\n",
    "z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s, outputs = forward(inputs_one_hot, h, c, params)\n",
    "\n",
    "output_sentence = [idx_to_word[np.argmax(output)] for output in outputs]\n",
    "print('Input sentence:')\n",
    "print(inputs)\n",
    "\n",
    "print('\\nTarget sequence:')\n",
    "print(targets)\n",
    "\n",
    "print('\\nPredicted sequence:')\n",
    "print([idx_to_word[np.argmax(output)] for output in outputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e07ba6",
   "metadata": {},
   "source": [
    "#### Backward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f2f0429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We get a loss of:\n",
      "2.7726512690013894\n"
     ]
    }
   ],
   "source": [
    "def backward(z, f, i, g, C, o, h, v, outputs, targets, p = params):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    z -- your concatenated input data  as a list of size m.\n",
    "    f -- your forget gate computations as a list of size m.\n",
    "    i -- your input gate computations as a list of size m.\n",
    "    g -- your candidate computations as a list of size m.\n",
    "    C -- your Cell states as a list of size m+1.\n",
    "    o -- your output gate computations as a list of size m.\n",
    "    h -- your Hidden state computations as a list of size m+1.\n",
    "    v -- your logit computations as a list of size m.\n",
    "    outputs -- your outputs as a list of size m.\n",
    "    targets -- your targets as a list of size m.\n",
    "    p -- python list containing:\n",
    "                        W_f -- Weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        b_f -- Bias of the forget gate, numpy array of shape (n_a, 1)\n",
    "                        W_i -- Weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        b_i -- Bias of the update gate, numpy array of shape (n_a, 1)\n",
    "                        W_g -- Weight matrix of the first \"tanh\", numpy array of shape (n_a, n_a + n_x)\n",
    "                        b_g --  Bias of the first \"tanh\", numpy array of shape (n_a, 1)\n",
    "                        W_o -- Weight matrix of the output gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        b_o --  Bias of the output gate, numpy array of shape (n_a, 1)\n",
    "                        W_v -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_v, n_a)\n",
    "                        b_v -- Bias relating the hidden-state to the output, numpy array of shape (n_v, 1)\n",
    "    Returns:\n",
    "    loss -- crossentropy loss for all elements in output\n",
    "    grads -- lists of gradients of every element in p\n",
    "    \"\"\"\n",
    "\n",
    "    # Unpack parameters\n",
    "    W_f, W_i, W_g, W_o, W_v, b_f, b_i, b_g, b_o, b_v = p\n",
    "\n",
    "    # Initialize gradients as zero\n",
    "    W_f_d = np.zeros_like(W_f)\n",
    "    b_f_d = np.zeros_like(b_f)\n",
    "\n",
    "    W_i_d = np.zeros_like(W_i)\n",
    "    b_i_d = np.zeros_like(b_i)\n",
    "\n",
    "    W_g_d = np.zeros_like(W_g)\n",
    "    b_g_d = np.zeros_like(b_g)\n",
    "\n",
    "    W_o_d = np.zeros_like(W_o)\n",
    "    b_o_d = np.zeros_like(b_o)\n",
    "\n",
    "    W_v_d = np.zeros_like(W_v)\n",
    "    b_v_d = np.zeros_like(b_v)\n",
    "    \n",
    "    # Set the next cell and hidden state equal to zero\n",
    "    dh_next = np.zeros_like(h[0])\n",
    "    dC_next = np.zeros_like(C[0])\n",
    "        \n",
    "    # Track loss\n",
    "    loss = 0\n",
    "    \n",
    "    for t in reversed(range(len(outputs))):\n",
    "        \n",
    "        # Compute the cross entropy\n",
    "        loss += -np.mean(np.log(outputs[t]) * targets[t])\n",
    "        # Get the previous hidden cell state\n",
    "        C_prev= C[t-1]\n",
    "        \n",
    "        # Compute the derivative of the relation of the hidden-state to the output gate\n",
    "        dv = np.copy(outputs[t])\n",
    "        dv[np.argmax(targets[t])] -= 1\n",
    "\n",
    "        # Update the gradient of the relation of the hidden-state to the output gate\n",
    "        W_v_d += np.dot(dv, h[t].T)\n",
    "        b_v_d += dv\n",
    "\n",
    "        # Compute the derivative of the hidden state and output gate\n",
    "        dh = np.dot(W_v.T, dv)        \n",
    "        dh += dh_next\n",
    "        do = dh * tanh(C[t])\n",
    "        do = sigmoid(o[t], derivative=True)*do\n",
    "        \n",
    "        # Update the gradients with respect to the output gate\n",
    "        W_o_d += np.dot(do, z[t].T)\n",
    "        b_o_d += do\n",
    "\n",
    "        # Compute the derivative of the cell state and candidate g\n",
    "        dC = np.copy(dC_next)\n",
    "        dC += dh * o[t] * tanh(tanh(C[t]), derivative=True)\n",
    "        dg = dC * i[t]\n",
    "        dg = tanh(g[t], derivative=True) * dg\n",
    "        \n",
    "        # Update the gradients with respect to the candidate\n",
    "        W_g_d += np.dot(dg, z[t].T)\n",
    "        b_g_d += dg\n",
    "\n",
    "        # Compute the derivative of the input gate and update its gradients\n",
    "        di = dC * g[t]\n",
    "        di = sigmoid(i[t], True) * di\n",
    "        W_i_d += np.dot(di, z[t].T)\n",
    "        b_i_d += di\n",
    "\n",
    "        # Compute the derivative of the forget gate and update its gradients\n",
    "        df = dC * C_prev\n",
    "        df = sigmoid(f[t]) * df\n",
    "        W_f_d += np.dot(df, z[t].T)\n",
    "        b_f_d += df\n",
    "\n",
    "        # Compute the derivative of the input and update the gradients of the previous hidden and cell state\n",
    "        dz = (np.dot(W_f.T, df)\n",
    "             + np.dot(W_i.T, di)\n",
    "             + np.dot(W_g.T, dg)\n",
    "             + np.dot(W_o.T, do))\n",
    "        dh_prev = dz[:hidden_size, :]\n",
    "        dC_prev = f[t] * dC\n",
    "        \n",
    "    grads= W_f_d, W_i_d, W_g_d, W_o_d, W_v_d, b_f_d, b_i_d, b_g_d, b_o_d, b_v_d\n",
    "    \n",
    "    # Clip gradients\n",
    "    grads = clip_gradient_norm(grads)\n",
    "    \n",
    "    return loss, grads\n",
    "\n",
    "\n",
    "# Perform a backward pass\n",
    "loss, grads = backward(z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s, outputs, targets_one_hot, params)\n",
    "\n",
    "print('We get a loss of:')\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3b41e52a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, training loss: 3.0793761865819556, validation loss: 4.29753830569197\n",
      "Epoch 10, training loss: 2.7590127032750336, validation loss: 2.8396692737708324\n",
      "Epoch 20, training loss: 1.4964384222305356, validation loss: 1.5586792178431634\n",
      "Epoch 30, training loss: 1.4024146066238103, validation loss: 1.429837543713019\n",
      "Epoch 40, training loss: 1.3922469528528612, validation loss: 1.4164724942657714\n",
      "Epoch 50, training loss: 1.387741757211103, validation loss: 1.4106886526588158\n",
      "Epoch 60, training loss: 1.3661140908473919, validation loss: 1.3924045916729344\n",
      "Epoch 70, training loss: 1.342458821601404, validation loss: 1.3735861169923682\n",
      "Epoch 80, training loss: 1.3401960609749899, validation loss: 1.3767730609207791\n",
      "Epoch 90, training loss: 1.3387989344700892, validation loss: 1.375256569199398\n",
      "Epoch 100, training loss: 1.337498918903253, validation loss: 1.3738925937007729\n",
      "Epoch 110, training loss: 1.3366153389516675, validation loss: 1.3733529502333008\n",
      "Epoch 120, training loss: 1.3360940759992166, validation loss: 1.373828213758021\n",
      "Epoch 130, training loss: 1.3358280906885944, validation loss: 1.3750962018066766\n",
      "Epoch 140, training loss: 1.3357572874304038, validation loss: 1.3769179282781625\n",
      "Epoch 150, training loss: 1.3358485092529824, validation loss: 1.3791350242695044\n",
      "Epoch 160, training loss: 1.336086056490968, validation loss: 1.381650486118022\n",
      "Epoch 170, training loss: 1.3364717402430557, validation loss: 1.3844035362608127\n",
      "Epoch 180, training loss: 1.3370268454786545, validation loss: 1.3873532568884483\n",
      "Epoch 190, training loss: 1.3377814483358421, validation loss: 1.3904644238224448\n",
      "Input sentence:\n",
      "['a', 'a', 'a', 'a', 'b', 'b', 'b', 'b']\n",
      "\n",
      "Target sequence:\n",
      "['a', 'a', 'a', 'b', 'b', 'b', 'b', 'EOS']\n",
      "\n",
      "Predicted sequence:\n",
      "['a', 'a', 'a', 'a', 'b', 'b', 'b', 'b']\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "num_epochs = 200\n",
    "\n",
    "# Initialize a new network\n",
    "z_size = hidden_size + vocab_size # Size of concatenated hidden + input vector\n",
    "params = init_lstm(hidden_size=hidden_size, vocab_size=vocab_size, z_size=z_size)\n",
    "\n",
    "# Initialize hidden state as zeros\n",
    "hidden_state = np.zeros((hidden_size, 1))\n",
    "\n",
    "# Track loss\n",
    "training_loss, validation_loss = [], []\n",
    "\n",
    "# For each epoch\n",
    "for i in range(num_epochs):\n",
    "    \n",
    "    # Track loss\n",
    "    epoch_training_loss = 0\n",
    "    epoch_validation_loss = 0\n",
    "    \n",
    "    # For each sentence in validation set\n",
    "    for inputs, targets in validation_set:\n",
    "        \n",
    "        # One-hot encode input and target sequence\n",
    "        inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size)\n",
    "        targets_one_hot = one_hot_encode_sequence(targets, vocab_size)\n",
    "\n",
    "        # Initialize hidden state and cell state as zeros\n",
    "        h = np.zeros((hidden_size, 1))\n",
    "        c = np.zeros((hidden_size, 1))\n",
    "\n",
    "        # Forward pass\n",
    "        z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s, outputs = forward(inputs_one_hot, h, c, params)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss, _ = backward(z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s, outputs, targets_one_hot, params)\n",
    "        \n",
    "        # Update loss\n",
    "        epoch_validation_loss += loss\n",
    "    \n",
    "    # For each sentence in training set\n",
    "    for inputs, targets in training_set:\n",
    "        \n",
    "        # One-hot encode input and target sequence\n",
    "        inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size)\n",
    "        targets_one_hot = one_hot_encode_sequence(targets, vocab_size)\n",
    "\n",
    "        # Initialize hidden state and cell state as zeros\n",
    "        h = np.zeros((hidden_size, 1))\n",
    "        c = np.zeros((hidden_size, 1))\n",
    "\n",
    "        # Forward pass\n",
    "        z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s, outputs = forward(inputs_one_hot, h, c, params)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss, grads = backward(z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s, outputs, targets_one_hot, params)\n",
    "        \n",
    "        # Update parameters\n",
    "        params = update_parameters(params, grads, lr=1e-1)\n",
    "        \n",
    "        # Update loss\n",
    "        epoch_training_loss += loss\n",
    "                \n",
    "    # Save loss for plot\n",
    "    training_loss.append(epoch_training_loss/len(training_set))\n",
    "    validation_loss.append(epoch_validation_loss/len(validation_set))\n",
    "\n",
    "    # Print loss every 10 epochs\n",
    "    if i % 10 == 0:\n",
    "        print(f'Epoch {i}, training loss: {training_loss[-1]}, validation loss: {validation_loss[-1]}')\n",
    "\n",
    "    \n",
    "# Get first sentence in test set\n",
    "inputs, targets = test_set[1]\n",
    "\n",
    "# One-hot encode input and target sequence\n",
    "inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size)\n",
    "targets_one_hot = one_hot_encode_sequence(targets, vocab_size)\n",
    "\n",
    "# Initialize hidden state as zeros\n",
    "h = np.zeros((hidden_size, 1))\n",
    "c = np.zeros((hidden_size, 1))\n",
    "\n",
    "# Forward pass\n",
    "z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s, outputs = forward(inputs_one_hot, h, c, params)\n",
    "\n",
    "# Print example\n",
    "print('Input sentence:')\n",
    "print(inputs)\n",
    "\n",
    "print('\\nTarget sequence:')\n",
    "print(targets)\n",
    "\n",
    "print('\\nPredicted sequence:')\n",
    "print([idx_to_word[np.argmax(output)] for output in outputs])\n",
    "\n",
    "# Plot training and validation loss\n",
    "epoch = np.arange(len(training_loss))\n",
    "plt.figure()\n",
    "plt.plot(epoch, training_loss, 'r', label='Training loss',)\n",
    "plt.plot(epoch, validation_loss, 'b', label='Validation loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch'), plt.ylabel('NLL')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416453c6",
   "metadata": {},
   "source": [
    "## PyTorch Implementation of LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8a3cf697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyRecurrentNet(\n",
      "  (lstm): LSTM(4, 50)\n",
      "  (l_out): Linear(in_features=50, out_features=4, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MyRecurrentNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyRecurrentNet, self).__init__()\n",
    "        \n",
    "        # Recurrent layer\n",
    "        self.lstm = nn.LSTM(input_size=vocab_size,\n",
    "                         hidden_size=50,\n",
    "                         num_layers=1,\n",
    "                         bidirectional=False)\n",
    "        \n",
    "        # Output layer\n",
    "        self.l_out = nn.Linear(in_features=50,\n",
    "                            out_features=vocab_size,\n",
    "                            bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # RNN returns output and last hidden state\n",
    "        x, (h, c) = self.lstm(x)\n",
    "        \n",
    "        # Flatten output for feed-forward layer\n",
    "        x = x.view(-1, self.lstm.hidden_size)\n",
    "        \n",
    "        # Output layer\n",
    "        x = self.l_out(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "net = MyRecurrentNet()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f4ada2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
