{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba18e621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A single sample from the generated dataset:\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Set seed such that we always get the same dataset\n",
    "# (this is a good idea in general)\n",
    "np.random.seed(42)\n",
    "\n",
    "def generate_dataset(num_sequences=2**8):\n",
    "    \"\"\"\n",
    "    Generates a number of sequences as our dataset.\n",
    "    \n",
    "    Args:\n",
    "     `num_sequences`: the number of sequences to be generated.\n",
    "     \n",
    "    Returns a list of sequences.\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "    \n",
    "    for _ in range(num_sequences): \n",
    "        num_tokens = np.random.randint(1, 12)\n",
    "        sample = ['a'] * num_tokens + ['b'] * num_tokens + ['EOS']\n",
    "        samples.append(sample)\n",
    "        \n",
    "    return samples\n",
    "\n",
    "\n",
    "sequences = generate_dataset()\n",
    "\n",
    "print('A single sample from the generated dataset:')\n",
    "print(sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44dfd4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 256 sentences and 4 unique tokens in our dataset (including UNK).\n",
      "\n",
      "The index of 'b' is 1\n",
      "The word corresponding to index 1 is 'b'\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def sequences_to_dicts(sequences):\n",
    "    flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "    \n",
    "    all_words = flatten(sequences)\n",
    "    \n",
    "    # Count number of word occurences\n",
    "    word_count = defaultdict(int)\n",
    "    for word in flatten(sequences):\n",
    "        word_count[word] += 1\n",
    "\n",
    "    # Sort by frequency\n",
    "    word_count = sorted(list(word_count.items()), key=lambda l: -l[1])\n",
    "\n",
    "    # Create a list of all unique words\n",
    "    unique_words = [item[0] for item in word_count]\n",
    "    \n",
    "    # Add UNK token to list of words\n",
    "    unique_words.append('UNK')\n",
    "\n",
    "    # Count number of sequences and number of unique words\n",
    "    num_sentences, vocab_size = len(sequences), len(unique_words)\n",
    "\n",
    "    # Create dictionaries so that we can go from word to index and back\n",
    "    # If a word is not in our vocabulary, we assign it to token 'UNK'\n",
    "    word_to_idx = defaultdict(lambda: vocab_size-1)\n",
    "    idx_to_word = defaultdict(lambda: 'UNK')\n",
    "\n",
    "    # Fill dictionaries\n",
    "    for idx, word in enumerate(unique_words):\n",
    "        # YOUR CODE HERE!\n",
    "        word_to_idx[word] = idx\n",
    "        idx_to_word[idx] = word\n",
    "\n",
    "    return word_to_idx, idx_to_word, num_sentences, vocab_size\n",
    "\n",
    "\n",
    "word_to_idx, idx_to_word, num_sequences, vocab_size = sequences_to_dicts(sequences)\n",
    "\n",
    "print(f'We have {num_sequences} sentences and {len(word_to_idx)} unique tokens in our dataset (including UNK).\\n')\n",
    "print('The index of \\'b\\' is', word_to_idx['b'])\n",
    "print(f'The word corresponding to index 1 is \\'{idx_to_word[1]}\\'')\n",
    "\n",
    "assert idx_to_word[word_to_idx['b']] == 'b', \\\n",
    "    'Consistency error: something went wrong in the conversion.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61fe29fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 204 samples in the training set.\n",
      "We have 25 samples in the validation set.\n",
      "We have 25 samples in the test set.\n"
     ]
    }
   ],
   "source": [
    "from torch.utils import data\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, inputs, targets):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the size of the dataset\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Retrieve inputs and targets at the given index\n",
    "        X = self.inputs[index]\n",
    "        y = self.targets[index]\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    \n",
    "def create_datasets(sequences, dataset_class, p_train=0.8, p_val=0.1, p_test=0.1):\n",
    "    # Define partition sizes\n",
    "    num_train = int(len(sequences)*p_train)\n",
    "    num_val = int(len(sequences)*p_val)\n",
    "    num_test = int(len(sequences)*p_test)\n",
    "\n",
    "    # Split sequences into partitions\n",
    "    sequences_train = sequences[:num_train]\n",
    "    sequences_val = sequences[num_train:num_train+num_val]\n",
    "    sequences_test = sequences[-num_test:]\n",
    "\n",
    "    def get_inputs_targets_from_sequences(sequences):\n",
    "        # Define empty lists\n",
    "        inputs, targets = [], []\n",
    "        \n",
    "        # Append inputs and targets s.t. both lists contain L-1 words of a sentence of length L\n",
    "        # but targets are shifted right by one so that we can predict the next word\n",
    "        for sequence in sequences:\n",
    "            inputs.append(sequence[:-1])\n",
    "            targets.append(sequence[1:])\n",
    "            \n",
    "        return inputs, targets\n",
    "\n",
    "    # Get inputs and targets for each partition\n",
    "    inputs_train, targets_train = get_inputs_targets_from_sequences(sequences_train)\n",
    "    inputs_val, targets_val = get_inputs_targets_from_sequences(sequences_val)\n",
    "    inputs_test, targets_test = get_inputs_targets_from_sequences(sequences_test)\n",
    "\n",
    "    # Create datasets\n",
    "    training_set = dataset_class(inputs_train, targets_train)\n",
    "    validation_set = dataset_class(inputs_val, targets_val)\n",
    "    test_set = dataset_class(inputs_test, targets_test)\n",
    "\n",
    "    return training_set, validation_set, test_set\n",
    "    \n",
    "\n",
    "training_set, validation_set, test_set = create_datasets(sequences, Dataset)\n",
    "\n",
    "print(f'We have {len(training_set)} samples in the training set.')\n",
    "print(f'We have {len(validation_set)} samples in the validation set.')\n",
    "print(f'We have {len(test_set)} samples in the test set.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a70cc9a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our one-hot encoding of 'a' has shape (4,).\n",
      "Our one-hot encoding of 'a b' has shape (2, 4, 1).\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(idx, vocab_size):\n",
    "    \"\"\"\n",
    "    One-hot encodes a single word given its index and the size of the vocabulary.\n",
    "    \n",
    "    Args:\n",
    "     `idx`: the index of the given word\n",
    "     `vocab_size`: the size of the vocabulary\n",
    "    \n",
    "    Returns a 1-D numpy array of length `vocab_size`.\n",
    "    \"\"\"\n",
    "    # Initialize the encoded array\n",
    "    one_hot = np.zeros(vocab_size)\n",
    "    \n",
    "    # Set the appropriate element to one\n",
    "    one_hot[idx] = 1.0\n",
    "\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "def one_hot_encode_sequence(sequence, vocab_size):\n",
    "    \"\"\"\n",
    "    One-hot encodes a sequence of words given a fixed vocabulary size.\n",
    "    \n",
    "    Args:\n",
    "     `sentence`: a list of words to encode\n",
    "     `vocab_size`: the size of the vocabulary\n",
    "     \n",
    "    Returns a 3-D numpy array of shape (num words, vocab size, 1).\n",
    "    \"\"\"\n",
    "    # Encode each word in the sentence\n",
    "    encoding = np.array([one_hot_encode(word_to_idx[word], vocab_size) for word in sequence])\n",
    "\n",
    "    # Reshape encoding s.t. it has shape (num words, vocab size, 1)\n",
    "    encoding = encoding.reshape(encoding.shape[0], encoding.shape[1], 1)\n",
    "    \n",
    "    return encoding\n",
    "\n",
    "\n",
    "test_word = one_hot_encode(word_to_idx['a'], vocab_size)\n",
    "print(f'Our one-hot encoding of \\'a\\' has shape {test_word.shape}.')\n",
    "\n",
    "test_sentence = one_hot_encode_sequence(['a', 'b'], vocab_size)\n",
    "print(f'Our one-hot encoding of \\'a b\\' has shape {test_sentence.shape}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9787373b",
   "metadata": {},
   "source": [
    "## RNN Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad782195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U: (50, 4)\n",
      "V: (50, 50)\n",
      "W: (4, 50)\n",
      "b_hidden: (50, 1)\n",
      "b_out: (4, 1)\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 50 # Number of dimensions in the hidden state\n",
    "vocab_size  = len(word_to_idx) # Size of the vocabulary used\n",
    "\n",
    "def init_orthogonal(param):\n",
    "    \"\"\"\n",
    "    Initializes weight parameters orthogonally.\n",
    "    This is a common initiailization for recurrent neural networks.\n",
    "    \n",
    "    Refer to this paper for an explanation of this initialization:\n",
    "    https://arxiv.org/abs/1312.6120\n",
    "    \"\"\"\n",
    "    if param.ndim < 2:\n",
    "        raise ValueError(\"Only parameters with 2 or more dimensions are supported.\")\n",
    "\n",
    "    rows, cols = param.shape\n",
    "    \n",
    "    new_param = np.random.randn(rows, cols)\n",
    "    \n",
    "    if rows < cols:\n",
    "        new_param = new_param.T\n",
    "    \n",
    "    # Compute QR factorization\n",
    "    q, r = np.linalg.qr(new_param)\n",
    "    \n",
    "    # Make Q uniform according to https://arxiv.org/pdf/math-ph/0609050.pdf\n",
    "    d = np.diag(r, 0)\n",
    "    ph = np.sign(d)\n",
    "    q *= ph\n",
    "\n",
    "    if rows < cols:\n",
    "        q = q.T\n",
    "    \n",
    "    new_param = q\n",
    "    \n",
    "    return new_param\n",
    "\n",
    "\n",
    "def init_rnn(hidden_size, vocab_size):\n",
    "    \"\"\"\n",
    "    Initializes our recurrent neural network.\n",
    "    \n",
    "    Args:\n",
    "     `hidden_size`: the dimensions of the hidden state\n",
    "     `vocab_size`: the dimensions of our vocabulary\n",
    "    \"\"\"\n",
    "    # Weight matrix (input to hidden state)\n",
    "    # YOUR CODE HERE!\n",
    "    U = np.zeros((hidden_size, vocab_size))\n",
    "\n",
    "    # Weight matrix (recurrent computation)\n",
    "    # YOUR CODE HERE!\n",
    "    V = np.zeros((hidden_size, hidden_size))\n",
    "\n",
    "    # Weight matrix (hidden state to output)\n",
    "    # YOUR CODE HERE!\n",
    "    W = np.zeros((vocab_size, hidden_size))\n",
    "\n",
    "    # Bias (hidden state)\n",
    "    # YOUR CODE HERE!\n",
    "    b_hidden = np.zeros((hidden_size, 1))\n",
    "\n",
    "    # Bias (output)\n",
    "    # YOUR CODE HERE!\n",
    "    b_out = np.zeros((vocab_size, 1))\n",
    "    \n",
    "    # Initialize weights\n",
    "    U = init_orthogonal(U)\n",
    "    V = init_orthogonal(V)\n",
    "    W = init_orthogonal(W)\n",
    "    \n",
    "    # Return parameters as a tuple\n",
    "    return U, V, W, b_hidden, b_out\n",
    "\n",
    "\n",
    "params = init_rnn(hidden_size=hidden_size, vocab_size=vocab_size)\n",
    "print('U:', params[0].shape)\n",
    "print('V:', params[1].shape)\n",
    "print('W:', params[2].shape)\n",
    "print('b_hidden:', params[3].shape)\n",
    "print('b_out:', params[4].shape)\n",
    "\n",
    "for param in params:\n",
    "    assert param.ndim == 2, \\\n",
    "        'all parameters should be 2-dimensional '\\\n",
    "        '(hint: a dimension can simply have size 1)'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549d858a",
   "metadata": {},
   "source": [
    "#### Implementing Basic Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5df95fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x, derivative=False):\n",
    "    \"\"\"\n",
    "    Computes the element-wise sigmoid activation function for an array x.\n",
    "\n",
    "    Args:\n",
    "     `x`: the array where the function is applied\n",
    "     `derivative`: if set to True will return the derivative instead of the forward pass\n",
    "    \"\"\"\n",
    "    x_safe = x + 1e-12\n",
    "    f = 1 / (1 + np.exp(-x_safe))\n",
    "    \n",
    "    if derivative: # Return the derivative of the function evaluated at x\n",
    "        return f * (1 - f)\n",
    "    else: # Return the forward pass of the function at x\n",
    "        return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd3ab80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x, derivative=False):\n",
    "    \"\"\"\n",
    "    Computes the element-wise tanh activation function for an array x.\n",
    "\n",
    "    Args:\n",
    "     `x`: the array where the function is applied\n",
    "     `derivative`: if set to True will return the derivative instead of the forward pass\n",
    "    \"\"\"\n",
    "    x_safe = x + 1e-12\n",
    "    f = (np.exp(x_safe)-np.exp(-x_safe))/(np.exp(x_safe)+np.exp(-x_safe))\n",
    "    \n",
    "    if derivative: # Return the derivative of the function evaluated at x\n",
    "        return 1-f**2\n",
    "    else: # Return the forward pass of the function at x\n",
    "        return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef066e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, derivative=False):\n",
    "    \"\"\"\n",
    "    Computes the softmax for an array x.\n",
    "    \n",
    "    Args:\n",
    "     `x`: the array where the function is applied\n",
    "     `derivative`: if set to True will return the derivative instead of the forward pass\n",
    "    \"\"\"\n",
    "    x_safe = x + 1e-12\n",
    "    f = np.exp(x_safe) / np.sum(np.exp(x_safe))\n",
    "    \n",
    "    if derivative: # Return the derivative of the function evaluated at x\n",
    "        pass # We will not need this one\n",
    "    else: # Return the forward pass of the function at x\n",
    "        return f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7172ff0d",
   "metadata": {},
   "source": [
    "#### Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46f3bdd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequence:\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b']\n",
      "\n",
      "Target sequence:\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS']\n",
      "\n",
      "Predicted sequence:\n",
      "['UNK', 'b', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'a', 'a', 'a', 'b', 'b', 'a', 'b']\n"
     ]
    }
   ],
   "source": [
    "def forward_pass(inputs, hidden_state, params):\n",
    "    \"\"\"\n",
    "    Computes the forward pass of a vanilla RNN.\n",
    "    \n",
    "    Args:\n",
    "     `inputs`: sequence of inputs to be processed\n",
    "     `hidden_state`: an already initialized hidden state\n",
    "     `params`: the parameters of the RNN\n",
    "    \"\"\"\n",
    "    # First we unpack our parameters\n",
    "    U, V, W, b_hidden, b_out = params\n",
    "    \n",
    "    # Create a list to store outputs and hidden states\n",
    "    outputs, hidden_states = [], []\n",
    "    \n",
    "    # For each element in input sequence\n",
    "    for t in range(len(inputs)):\n",
    "\n",
    "        # Compute new hidden state\n",
    "        # YOUR CODE HERE!\n",
    "        hidden_state_prev = hidden_states[t-1] if t > 0 else hidden_state\n",
    "        hidden_state = tanh(np.dot(U, inputs[t]) + np.dot(V, hidden_state_prev) + b_hidden)\n",
    "\n",
    "        # Compute output\n",
    "        # YOUR CODE HERE!\n",
    "        out = softmax(np.dot(W, hidden_state) + b_out)\n",
    "        \n",
    "        # Save results and continue\n",
    "        outputs.append(out)\n",
    "        hidden_states.append(hidden_state.copy())\n",
    "    \n",
    "    return outputs, hidden_states\n",
    "\n",
    "\n",
    "# Get first sequence in training set\n",
    "test_input_sequence, test_target_sequence = training_set[0]\n",
    "\n",
    "# One-hot encode input and target sequence\n",
    "test_input = one_hot_encode_sequence(test_input_sequence, vocab_size)\n",
    "test_target = one_hot_encode_sequence(test_target_sequence, vocab_size)\n",
    "\n",
    "# Initialize hidden state as zeros\n",
    "hidden_state = np.zeros((hidden_size, 1))\n",
    "\n",
    "# Now let's try out our new function\n",
    "outputs, hidden_states = forward_pass(test_input, hidden_state, params)\n",
    "\n",
    "print('Input sequence:')\n",
    "print(test_input_sequence)\n",
    "\n",
    "print('\\nTarget sequence:')\n",
    "print(test_target_sequence)\n",
    "\n",
    "print('\\nPredicted sequence:')\n",
    "print([idx_to_word[np.argmax(output)] for output in outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "953126b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_gradient_norm(grads, max_norm=0.25):\n",
    "    \"\"\"\n",
    "    Clips gradients to have a maximum norm of `max_norm`.\n",
    "    This is to prevent the exploding gradients problem.\n",
    "    \"\"\" \n",
    "    # Set the maximum of the norm to be of type float\n",
    "    max_norm = float(max_norm)\n",
    "    total_norm = 0\n",
    "    \n",
    "    # Calculate the L2 norm squared for each gradient and add them to the total norm\n",
    "    for grad in grads:\n",
    "        grad_norm = np.sum(np.power(grad, 2))\n",
    "        total_norm += grad_norm\n",
    "    \n",
    "    total_norm = np.sqrt(total_norm)\n",
    "    \n",
    "    # Calculate clipping coeficient\n",
    "    clip_coef = max_norm / (total_norm + 1e-6)\n",
    "    \n",
    "    # If the total norm is larger than the maximum allowable norm, then clip the gradient\n",
    "    if clip_coef < 1:\n",
    "        for grad in grads:\n",
    "            grad *= clip_coef\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b59a8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We get a loss of:\n",
      "19.442576294993177\n"
     ]
    }
   ],
   "source": [
    "def backward_pass(inputs, outputs, hidden_states, targets, params):\n",
    "    \"\"\"\n",
    "    Computes the backward pass of a vanilla RNN.\n",
    "    \n",
    "    Args:\n",
    "     `inputs`: sequence of inputs to be processed\n",
    "     `outputs`: sequence of outputs from the forward pass\n",
    "     `hidden_states`: sequence of hidden_states from the forward pass\n",
    "     `targets`: sequence of targets\n",
    "     `params`: the parameters of the RNN\n",
    "    \"\"\"\n",
    "    # First we unpack our parameters\n",
    "    U, V, W, b_hidden, b_out = params\n",
    "    \n",
    "    # Initialize gradients as zero\n",
    "    d_U, d_V, d_W = np.zeros_like(U), np.zeros_like(V), np.zeros_like(W)\n",
    "    d_b_hidden, d_b_out = np.zeros_like(b_hidden), np.zeros_like(b_out)\n",
    "    \n",
    "    # Keep track of hidden state derivative and loss\n",
    "    d_h_next = np.zeros_like(hidden_states[0])\n",
    "    loss = 0\n",
    "    \n",
    "    # For each element in output sequence\n",
    "    # NB: We iterate backwards s.t. t = N, N-1, ... 1, 0\n",
    "    for t in reversed(range(len(outputs))):\n",
    "\n",
    "        # Compute cross-entropy loss (as a scalar)\n",
    "        loss += -1*np.log(outputs[t][np.argmax(targets[t])][0])\n",
    "        \n",
    "        d_o = outputs[t].copy()\n",
    "        d_o[np.argmax(targets[t])] -= 1\n",
    "        \n",
    "        d_W += np.dot(d_o, hidden_states[t].T)\n",
    "        d_b_out += d_o\n",
    "        \n",
    "        d_h = np.dot(W.T, d_o) + d_h_next\n",
    "        \n",
    "        d_f = (1 - hidden_states[t]**2) * d_h\n",
    "        d_b_hidden += d_f\n",
    "        \n",
    "        d_U += np.dot(d_f, inputs[t].T)\n",
    "        \n",
    "        d_V += np.dot(d_f, hidden_states[t-1].T)\n",
    "        d_h_next = np.dot(V.T, d_f)\n",
    "    \n",
    "    # Pack gradients\n",
    "    grads = d_U, d_V, d_W, d_b_hidden, d_b_out    \n",
    "    \n",
    "    # Clip gradients\n",
    "    grads = clip_gradient_norm(grads)\n",
    "    \n",
    "    return loss, grads\n",
    "        \n",
    "\n",
    "\n",
    "loss, grads = backward_pass(test_input, outputs, hidden_states, test_target, params)\n",
    "\n",
    "print('We get a loss of:')\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6017518b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(params, grads, lr=1e-3):\n",
    "    # Take a step\n",
    "    for param, grad in zip(params, grads):\n",
    "        param -= lr * grad\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fc5911",
   "metadata": {},
   "source": [
    "#### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b36a7589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, training loss: 15.910508980828963, validation loss: 16.77082373423666\n",
      "Epoch 100, training loss: 5.842587609551899, validation loss: 5.961775262985713\n",
      "Epoch 200, training loss: 4.837018727735779, validation loss: 5.1858526349135605\n",
      "Epoch 300, training loss: 4.597945355563288, validation loss: 5.042666304439337\n",
      "Epoch 400, training loss: 4.614525975998632, validation loss: 5.358800478112993\n",
      "Epoch 500, training loss: 4.688253280743628, validation loss: 5.525259372086577\n",
      "Epoch 600, training loss: 4.623338573450351, validation loss: 5.280416544872708\n",
      "Epoch 700, training loss: 4.432645988520303, validation loss: 5.06703114140339\n",
      "Epoch 800, training loss: 4.316693612553475, validation loss: 4.9507236393319065\n",
      "Epoch 900, training loss: 4.216705015299984, validation loss: 4.816942413797928\n",
      "Input sentence:\n",
      "['a', 'a', 'a', 'a', 'b', 'b', 'b', 'b']\n",
      "\n",
      "Target sequence:\n",
      "['a', 'a', 'a', 'b', 'b', 'b', 'b', 'EOS']\n",
      "\n",
      "Predicted sequence:\n",
      "['b', 'a', 'a', 'a', 'b', 'b', 'b', 'EOS']\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Hyper-parameters\n",
    "num_epochs = 1000\n",
    "\n",
    "# Initialize a new network\n",
    "params = init_rnn(hidden_size=hidden_size, vocab_size=vocab_size)\n",
    "\n",
    "# Initialize hidden state as zeros\n",
    "hidden_state = np.zeros((hidden_size, 1))\n",
    "\n",
    "# Track loss\n",
    "training_loss, validation_loss = [], []\n",
    "\n",
    "# For each epoch\n",
    "for i in range(num_epochs):\n",
    "    \n",
    "    # Track loss\n",
    "    epoch_training_loss = 0\n",
    "    epoch_validation_loss = 0\n",
    "    \n",
    "     # For each sentence in validation set\n",
    "    for inputs, targets in validation_set:\n",
    "        \n",
    "        # One-hot encode input and target sequence\n",
    "        inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size)\n",
    "        targets_one_hot = one_hot_encode_sequence(targets, vocab_size)\n",
    "        \n",
    "        # Re-initialize hidden state\n",
    "        hidden_state = np.zeros_like(hidden_state)\n",
    "\n",
    "        # Forward pass\n",
    "        # YOUR CODE HERE!\n",
    "        outputs, hidden_states = forward_pass(inputs_one_hot, hidden_state, params)\n",
    "\n",
    "        # Backward pass\n",
    "        # YOUR CODE HERE!\n",
    "        loss, _ = backward_pass(inputs_one_hot, outputs, hidden_states, targets_one_hot, params)\n",
    "        \n",
    "        # Update loss\n",
    "        epoch_validation_loss += loss\n",
    "    \n",
    "    # For each sentence in training set\n",
    "    for inputs, targets in training_set:\n",
    "        \n",
    "        # One-hot encode input and target sequence\n",
    "        inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size)\n",
    "        targets_one_hot = one_hot_encode_sequence(targets, vocab_size)\n",
    "        \n",
    "        # Re-initialize hidden state\n",
    "        hidden_state = np.zeros_like(hidden_state)\n",
    "\n",
    "        # Forward pass\n",
    "        # YOUR CODE HERE!\n",
    "        outputs, hidden_states = forward_pass(inputs_one_hot, hidden_state, params)\n",
    "\n",
    "        # Backward pass\n",
    "        # YOUR CODE HERE!\n",
    "        loss, grads = backward_pass(inputs_one_hot, outputs, hidden_states, targets_one_hot, params)\n",
    "        \n",
    "        if np.isnan(loss):\n",
    "            raise ValueError('Gradients have vanished/exploded!')\n",
    "        \n",
    "        # Update parameters\n",
    "        # YOUR CODE HERE!\n",
    "        params = update_parameters(params, grads)\n",
    "        \n",
    "        # Update loss\n",
    "        epoch_training_loss += loss\n",
    "        \n",
    "    # Save loss for plot\n",
    "    training_loss.append(epoch_training_loss/len(training_set))\n",
    "    validation_loss.append(epoch_validation_loss/len(validation_set))\n",
    "\n",
    "    # Print loss every 100 epochs\n",
    "    if i % 100 == 0:\n",
    "        print(f'Epoch {i}, training loss: {training_loss[-1]}, validation loss: {validation_loss[-1]}')\n",
    "\n",
    "\n",
    "# Get first sentence in test set\n",
    "inputs, targets = test_set[1]\n",
    "\n",
    "# One-hot encode input and target sequence\n",
    "inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size)\n",
    "targets_one_hot = one_hot_encode_sequence(targets, vocab_size)\n",
    "\n",
    "# Initialize hidden state as zeros\n",
    "hidden_state = np.zeros((hidden_size, 1))\n",
    "\n",
    "# Forward pass\n",
    "outputs, hidden_states = forward_pass(inputs_one_hot, hidden_state, params)\n",
    "output_sentence = [idx_to_word[np.argmax(output)] for output in outputs]\n",
    "print('Input sentence:')\n",
    "print(inputs)\n",
    "\n",
    "print('\\nTarget sequence:')\n",
    "print(targets)\n",
    "\n",
    "print('\\nPredicted sequence:')\n",
    "print([idx_to_word[np.argmax(output)] for output in outputs])\n",
    "\n",
    "# Plot training and validation loss\n",
    "epoch = np.arange(len(training_loss))\n",
    "plt.figure()\n",
    "plt.plot(epoch, training_loss, 'r', label='Training loss',)\n",
    "plt.plot(epoch, validation_loss, 'b', label='Validation loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch'), plt.ylabel('NLL')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99d0c2f",
   "metadata": {},
   "source": [
    "#### Extrapolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1903ae3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 0: a a b\n",
      "Predicted sequence: ['a', 'a', 'b', 'b', 'EOS']\n",
      "\n",
      "Example 1: a a a a b\n",
      "Predicted sequence: ['a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'a', 'a', 'a', 'a', 'a', 'a']\n",
      "\n",
      "Example 2: a a a a a a b\n",
      "Predicted sequence: ['a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS']\n",
      "\n",
      "Example 3: a\n",
      "Predicted sequence: ['a', 'b', 'EOS']\n",
      "\n",
      "Example 4: r n n\n",
      "Predicted sequence: ['r', 'n', 'n', 'EOS', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def freestyle(params, sentence='', num_generate=10):\n",
    "    \"\"\"\n",
    "    Takes in a sentence as a string and outputs a sequence\n",
    "    based on the predictions of the RNN.\n",
    "    \n",
    "    Args:\n",
    "     `params`: the parameters of the network\n",
    "     `sentence`: string with whitespace-separated tokens\n",
    "     `num_generate`: the number of tokens to generate\n",
    "    \"\"\"\n",
    "    sentence = sentence.split(' ')\n",
    "    \n",
    "    sentence_one_hot = one_hot_encode_sequence(sentence, vocab_size)\n",
    "    \n",
    "    # Initialize hidden state as zeros\n",
    "    hidden_state = np.zeros((hidden_size, 1))\n",
    "\n",
    "    # Generate hidden state for sentence\n",
    "    outputs, hidden_states = forward_pass(sentence_one_hot, hidden_state, params)\n",
    "    \n",
    "    # Output sentence\n",
    "    output_sentence = sentence\n",
    "    \n",
    "    # Append first prediction\n",
    "    word = idx_to_word[np.argmax(outputs[-1])]    \n",
    "    output_sentence.append(word)\n",
    "    \n",
    "    # Forward pass\n",
    "    for i in range(num_generate):\n",
    "\n",
    "        # Get the latest prediction and latest hidden state\n",
    "        output = outputs[-1]\n",
    "        hidden_state = hidden_states[-1]\n",
    "    \n",
    "        # Reshape our output to match the input shape of our forward pass\n",
    "        output = output.reshape(1, output.shape[0], output.shape[1])\n",
    "    \n",
    "        # Forward pass\n",
    "        outputs, hidden_states = forward_pass(output, hidden_state, params)\n",
    "        \n",
    "        # Compute the index of the most likely word and look up the corresponding word\n",
    "        word = idx_to_word[np.argmax(outputs)]\n",
    "        \n",
    "        output_sentence.append(word)\n",
    "        \n",
    "        if word == 'EOS':\n",
    "            break\n",
    "        \n",
    "    return output_sentence\n",
    "\n",
    "\n",
    "# Perform freestyle (extrapolation)\n",
    "test_examples = ['a a b', 'a a a a b', 'a a a a a a b', 'a', 'r n n']\n",
    "for i, test_example in enumerate(test_examples):\n",
    "    print(f'Example {i}:', test_example)\n",
    "    print('Predicted sequence:', freestyle(params, sentence=test_example), end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfcb7d6",
   "metadata": {},
   "source": [
    "## LSTMs\n",
    "\n",
    "A vanilla RNN suffers from vanishing gradient problem. So, the researchers turned their attention to LSTMs. There are three types of gates in LSTMs:\n",
    "\n",
    "<ul>\n",
    "    <li> Forget Gate: This gate decides which information needs attention and which can be ignored. </li>\n",
    "    <li> Input Gate </li>\n",
    "    <li> Output Gate </li>\n",
    "</ul>\n",
    "\n",
    "<img src=\"LSTM.png\" />\n",
    "<img src=\"LSTM2.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a815ce0",
   "metadata": {},
   "source": [
    "#### Initialization of LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62a014b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_f: (50, 54)\n",
      "W_i: (50, 54)\n",
      "W_g: (50, 54)\n",
      "W_o: (50, 54)\n",
      "W_v: (4, 50)\n",
      "b_i: (50, 1)\n",
      "b_g: (50, 1)\n",
      "b_o: (50, 1)\n",
      "b_v: (50, 1)\n"
     ]
    }
   ],
   "source": [
    "# Size of concatenated hidden + input vector\n",
    "z_size = hidden_size + vocab_size \n",
    "\n",
    "def init_lstm(hidden_size, vocab_size, z_size):\n",
    "    \"\"\"\n",
    "    Initializes our LSTM network.\n",
    "    \n",
    "    Args:\n",
    "     `hidden_size`: the dimensions of the hidden state\n",
    "     `vocab_size`: the dimensions of our vocabulary\n",
    "     `z_size`: the dimensions of the concatenated input \n",
    "    \"\"\"\n",
    "    # Weight matrix (forget gate)\n",
    "    # YOUR CODE HERE!\n",
    "    W_f = np.zeros((hidden_size, z_size))\n",
    "    \n",
    "    # Bias for forget gate\n",
    "    b_f = np.zeros((hidden_size, 1))\n",
    "\n",
    "    # Weight matrix (input gate)\n",
    "    # YOUR CODE HERE!\n",
    "    W_i = np.zeros((hidden_size, z_size))\n",
    "    \n",
    "    # Bias for input gate\n",
    "    b_i = np.zeros((hidden_size, 1))\n",
    "\n",
    "    # Weight matrix (candidate)\n",
    "    # YOUR CODE HERE!\n",
    "    W_g = np.zeros((hidden_size, z_size))\n",
    "    \n",
    "    # Bias for candidate\n",
    "    b_g = np.zeros((hidden_size, 1))\n",
    "\n",
    "    # Weight matrix of the output gate\n",
    "    # YOUR CODE HERE!\n",
    "    W_o = np.zeros((hidden_size, z_size))\n",
    "    \n",
    "    # Bias for output gate\n",
    "    b_o = np.zeros((hidden_size, 1))\n",
    "\n",
    "    # Weight matrix relating the hidden-state to the output\n",
    "    # YOUR CODE HERE!\n",
    "    W_v = np.zeros((vocab_size, hidden_size))\n",
    "    \n",
    "    # Bias for logits\n",
    "    b_v = np.zeros((vocab_size, 1))\n",
    "    \n",
    "    # Initialize weights according to https://arxiv.org/abs/1312.6120\n",
    "    W_f = init_orthogonal(W_f)\n",
    "    W_i = init_orthogonal(W_i)\n",
    "    W_g = init_orthogonal(W_g)\n",
    "    W_o = init_orthogonal(W_o)\n",
    "    W_v = init_orthogonal(W_v)\n",
    "\n",
    "    return W_f, W_i, W_g, W_o, W_v, b_f, b_i, b_g, b_o, b_v\n",
    "\n",
    "\n",
    "params = init_lstm(hidden_size=hidden_size, vocab_size=vocab_size, z_size=z_size)\n",
    "print('W_f:', params[0].shape)\n",
    "print('W_i:', params[1].shape)\n",
    "print('W_g:', params[2].shape)\n",
    "print('W_o:', params[3].shape)\n",
    "print('W_v:', params[4].shape)\n",
    "print('b_i:', params[5].shape)\n",
    "print('b_g:', params[6].shape)\n",
    "print('b_o:', params[7].shape)\n",
    "print('b_v:', params[8].shape)\n",
    "\n",
    "for param in params:\n",
    "    assert param.ndim == 2, \\\n",
    "        'all parameters should be 2-dimensional '\\\n",
    "        '(hint: a dimension can simply have size 1)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4167bef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
